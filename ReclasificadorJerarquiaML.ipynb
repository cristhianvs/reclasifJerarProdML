{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67471901-4e6e-4261-9c7d-07d1ad76a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn torch transformers openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3ce60-e3d1-453d-bdf5-a5bf6596917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2: Importación de librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import torch\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from openpyxl import load_workbook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Celda 3: Funciones de utilidad\n",
    "def limpiar_texto(texto: str) -> str:\n",
    "    \"\"\"Limpia y normaliza texto\"\"\"\n",
    "    if not isinstance(texto, str):\n",
    "        return ''\n",
    "    \n",
    "    texto = texto.lower()\n",
    "    texto = unicodedata.normalize('NFD', texto)\n",
    "    texto = texto.encode('ascii', 'ignore').decode('utf-8')\n",
    "    texto = re.sub(r'[^a-z0-9\\s]', '', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    \n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8385228f-df82-4247-a551-cf09c89d51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: Clase para embeddings\n",
    "class HFEmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name=\"dccuchile/bert-base-spanish-wwm-cased\", \n",
    "                 device=None, max_length=128, batch_size=32):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        print(f\"Usando dispositivo: {self.device}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        all_embeddings = []\n",
    "        total_batches = len(X) // self.batch_size + (1 if len(X) % self.batch_size != 0 else 0)\n",
    "        \n",
    "        print(f\"Procesando {len(X)} textos en {total_batches} batches...\")\n",
    "        for i in range(0, len(X), self.batch_size):\n",
    "            print(f\"Batch {i//self.batch_size + 1}/{total_batches}\", end='\\r')\n",
    "            batch_texts = X[i:i + self.batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                list(batch_texts),\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f18f3d-e44f-40b6-b61a-eed44d3d9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5: Función para split de datos\n",
    "def asegurar_clases_en_entrenamiento(df, y_cols, frac_train=0.70, frac_val=0.15, \n",
    "                                   random_state=42):\n",
    "    \"\"\"Particiona el dataset garantizando representación de clases\"\"\"\n",
    "    df_train, df_temp = train_test_split(df, test_size=(1-frac_train), \n",
    "                                        random_state=random_state)\n",
    "    \n",
    "    frac_val_relativo = frac_val / (1-frac_train)\n",
    "    df_val, df_test = train_test_split(df_temp, test_size=(1-frac_val_relativo), \n",
    "                                      random_state=random_state)\n",
    "\n",
    "    for col in y_cols:\n",
    "        categorias_entrenamiento = set(df_train[col].unique())\n",
    "        todas_categorias = set(df[col].unique())\n",
    "        faltantes = todas_categorias - categorias_entrenamiento\n",
    "        \n",
    "        if faltantes:\n",
    "            print(f\"Ajustando categorías faltantes para {col}: {faltantes}\")\n",
    "            for cat in faltantes:\n",
    "                candidatos = df_val[df_val[col] == cat]\n",
    "                if not candidatos.empty:\n",
    "                    df_train = pd.concat([df_train, candidatos.iloc[[0]]])\n",
    "                    df_val = df_val.drop(candidatos.iloc[[0]].index)\n",
    "                else:\n",
    "                    candidatos = df_test[df_test[col] == cat]\n",
    "                    if not candidatos.empty:\n",
    "                        df_train = pd.concat([df_train, candidatos.iloc[[0]]])\n",
    "                        df_test = df_test.drop(candidatos.iloc[[0]].index)\n",
    "\n",
    "    print(f\"\\nDistribución final:\")\n",
    "    print(f\"Training: {len(df_train)}\")\n",
    "    print(f\"Validación: {len(df_val)}\")\n",
    "    print(f\"Test: {len(df_test)}\")\n",
    "    \n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "# Celda 6: Función para evaluación\n",
    "def evaluar_modelo(pipeline, X, y, nombre_conjunto='Test'):\n",
    "    \"\"\"Evalúa el modelo y muestra métricas por columna\"\"\"\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    y_pred = pipeline.predict(X)\n",
    "    \n",
    "    print(f\"\\nEvaluación - {nombre_conjunto}:\")\n",
    "    \n",
    "    for i, col in enumerate(y.columns):\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(classification_report(y.iloc[:, i], y_pred[:, i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc92c02-81d1-49c3-945a-fb935a3cafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 7: Carga y preprocesamiento\n",
    "# Cargar datos de un solo archivo\n",
    "df = pd.read_csv(\"datos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe3c8a-bb63-4960-aa17-fc85402b7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar texto y manejar NULL\n",
    "df['SUBLINEA NUEVA'] = df['SUBLINEA NUEVA'].fillna(\"NULL\")\n",
    "\n",
    "# Limpiar columnas de texto\n",
    "columnas_a_limpiar = ['Descripcion', 'DepartamentoTroncal', 'DepartamentoSAP', 'Linea']\n",
    "for col in columnas_a_limpiar:\n",
    "    df[col] = df[col].fillna('').apply(limpiar_texto)\n",
    "\n",
    "# Crear texto de entrada\n",
    "df['TextoEntrada'] = (\n",
    "    df['Descripcion'] + ' ' +\n",
    "    df['DepartamentoTroncal'] + ' ' +\n",
    "    df['DepartamentoSAP'] + ' ' +\n",
    "    df['Linea']\n",
    ").str.strip()\n",
    "\n",
    "# Mantener solo columnas necesarias\n",
    "cols_necesarias = ['SkuID', 'TextoEntrada', 'DEPARTAMENTO NUEVO', \n",
    "                   'SUBDEPARTAMENTO NUEVO', 'LINEA NUEVA', 'SUBLINEA NUEVA']\n",
    "df = df[cols_necesarias]\n",
    "\n",
    "# Celda 8: Preparación para entrenamiento\n",
    "# Definir columnas objetivo\n",
    "Y_cols = ['DEPARTAMENTO NUEVO', 'SUBDEPARTAMENTO NUEVO', \n",
    "          'LINEA NUEVA', 'SUBLINEA NUEVA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade11d3a-7e33-422b-8957-76336653a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que no queden nulos\n",
    "print(\"Filas después de eliminar nulos:\", len(df))\n",
    "print(\"Nulos restantes por columna:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d168a6-575e-4237-b799-ef920755ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columnas disponibles:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25fffd-d46f-408b-8365-f98e92484cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb37908-a595-4076-8492-7a4c813ef171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split de datos\n",
    "df_train, df_val, df_test = asegurar_clases_en_entrenamiento(df, Y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480b336-c78d-4c6b-a8f5-736578db2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar X e Y\n",
    "X_train = df_train['TextoEntrada']\n",
    "Y_train = df_train[Y_cols]\n",
    "\n",
    "X_val = df_val['TextoEntrada']\n",
    "Y_val = df_val[Y_cols]\n",
    "\n",
    "X_test = df_test['TextoEntrada']\n",
    "Y_test = df_test[Y_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969f9f3-ead0-4437-80ce-6243e7ad3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 9: Entrenamiento - OPCION INDIVIDUAL\n",
    "# Crear pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('embeddings', HFEmbeddingTransformer(batch_size=16)),\n",
    "    ('clf', MultiOutputClassifier(LogisticRegression(max_iter=1000)))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d8d1e-b59e-47c2-b981-763a1e2df6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 9: Entrenamiento - OPCION GRID\n",
    "def crear_pipeline_con_grid():\n",
    "    pipeline = Pipeline([\n",
    "        ('embeddings', HFEmbeddingTransformer()),\n",
    "        ('clf', MultiOutputClassifier(LogisticRegression()))\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'embeddings__max_length': [128, 256],\n",
    "        'embeddings__batch_size': [8, 16],\n",
    "        'embeddings__model_name': [\n",
    "            'dccuchile/bert-base-spanish-wwm-cased',\n",
    "            'PlanTL-GOB-ES/roberta-base-bne'\n",
    "        ],\n",
    "        'clf__estimator__C': [0.1, 1.0, 10.0],\n",
    "        'clf__estimator__max_iter': [1000, 2000],\n",
    "        'clf__estimator__class_weight': [None, 'balanced'],\n",
    "        'clf__estimator__solver': ['lbfgs', 'saga']\n",
    "    }\n",
    "    \n",
    "    return pipeline, param_grid\n",
    "\n",
    "def ejecutar_grid_search(X_train, y_train, n_jobs=-1, cv=3):\n",
    "    pipeline, param_grid = crear_pipeline_con_grid()\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=2,\n",
    "        scoring='f1_weighted'\n",
    "    )\n",
    "    \n",
    "    print(\"Iniciando búsqueda de hiperparámetros...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nMejores parámetros encontrados:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"\\nMejor puntuación: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Replace the training cell content with:\n",
    "print(\"Iniciando búsqueda de hiperparámetros y entrenamiento...\")\n",
    "pipeline = ejecutar_grid_search(X_train, Y_train, n_jobs=4, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92efba-bcc1-4f51-997f-ed95b3c8760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 10: Evaluación\n",
    "evaluar_modelo(pipeline, X_val, Y_val, 'Validación')\n",
    "evaluar_modelo(pipeline, X_test, Y_test, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33450238-b720-4eeb-9950-0b40cd27fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 11: Guardar modelo (opcional)\n",
    "import joblib\n",
    "joblib.dump(pipeline, 'modelo_clasificacion_productos.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e96fd-7056-4aa9-b19f-e68828e9c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 12: Predicción con archivo Excel\n",
    "def predecir_desde_excel(pipeline, input_file):\n",
    "   df_pred = pd.read_excel(input_file)\n",
    "   \n",
    "   df_pred['TextoEntrada'] = (\n",
    "       df_pred['Descripcion'].fillna('') + ' ' +\n",
    "       df_pred['DepartamentoTroncal'].fillna('') + ' ' +\n",
    "       df_pred['DepartamentoSAP'].fillna('') + ' ' +\n",
    "       df_pred['Linea'].fillna('')\n",
    "   ).apply(limpiar_texto)\n",
    "   \n",
    "   predicciones = pipeline.predict(df_pred['TextoEntrada'])\n",
    "   \n",
    "   for i, col in enumerate(Y_cols):\n",
    "       df_pred[col] = predicciones[:, i]\n",
    "   \n",
    "   # Eliminar columna TextoEntrada\n",
    "   df_pred.drop('TextoEntrada', axis=1, inplace=True)\n",
    "   \n",
    "   output_file = 'datos_predichos.xlsx'\n",
    "   df_pred.to_excel(output_file, index=False)\n",
    "   print(f\"Predicciones guardadas en: {output_file}\")\n",
    "   \n",
    "   return df_pred\n",
    "\n",
    "# Ejecutar predicciones\n",
    "df_predicciones = predecir_desde_excel(pipeline, 'datos_a_predecir.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c6ccc-f024-4ca5-88e1-fd07d92ac299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
